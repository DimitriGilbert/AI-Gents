#!/bin/bash
#
# API library for AI-Gents
# HTTP client with connection pooling and retry logic
# Uses curl's cookie jar for HTTP keep-alive
#

# =============================================================================
# CONFIGURATION CONSTANTS
# =============================================================================

# Connection keep-alive time in seconds
# Balance between connection reuse and freshness:
# - Too low (< 60s): Frequent reconnections, higher latency
# - Too high (> 180s): Stale connections may cause errors
# - 120s is chosen based on typical API server timeouts
readonly API_KEEPALIVE_TIME=120

# Default request timeout in seconds
# Should be long enough for large LLM responses
# but short enough to fail fast on network issues
readonly API_DEFAULT_TIMEOUT=300

# Maximum number of retry attempts for failed requests
# Exponential backoff: 1s, 2s, 4s between retries
readonly API_MAX_RETRIES=3

# Initial retry delay in seconds
readonly API_RETRY_DELAY=1

# Streaming batch interval in nanoseconds for 60fps target
# 16ms = 16000000ns = ~60fps
# Lower values = smoother but more CPU usage
# Higher values = less CPU but more choppy output
readonly STREAM_BATCH_INTERVAL_NS=16000000

# Connection pool directory
_API_CONN_POOL_DIR="${TMPDIR:-/tmp}/ai-gents-conn"

# Initialize connection pool
api_init_pool() {
    mkdir -p "$_API_CONN_POOL_DIR"
}

# Get connection identifier for a host
# Usage: _api_get_conn_id <url>
_api_get_conn_id() {
    local url="$1"
    # Extract host:port from URL
    local host
    if [[ "$url" =~ ^https?://([^/]+) ]]; then
        host="${BASH_REMATCH[1]}"
    else
        host="$url"
    fi
    # Sanitize for filename
    echo "$host" | tr '/:' '_'
}

# Build curl command with connection pooling
# Usage: api_curl_cmd <endpoint_url>
# Returns: array of curl arguments via nameref
api_curl_cmd() {
    local endpoint="$1"
    local -n curl_args="$2"
    local conn_id
    conn_id="$(_api_get_conn_id "$endpoint")"
    local cookie_jar="$_API_CONN_POOL_DIR/${conn_id}.jar"
    
    # Enhanced connection pooling for HTTP keep-alive
    # shellcheck disable=SC2034  # curl_args is used via nameref
    curl_args=(
        -s  # silent
        -X POST
        -H "Content-Type: application/json"
        -H "Connection: keep-alive"
        --cookie-jar "$cookie_jar"
        --cookie "$cookie_jar"
        --keepalive-time "$API_KEEPALIVE_TIME"
        --max-time "$API_DEFAULT_TIMEOUT"
        --retry 0  # We handle retries manually for better control
        --http1.1  # Force HTTP/1.1 for better keep-alive compatibility
    )
}

# Make API request with retry logic
# Usage: api_request <endpoint> <payload_file> <api_key> [timeout]
# Returns: HTTP response body via stdout, exit code reflects success/failure
api_request() {
    local endpoint="$1"
    local payload_file="$2"
    local api_key="$3"
    local timeout="${4:-$API_DEFAULT_TIMEOUT}"
    local max_retries="$API_MAX_RETRIES"
    local retry_delay="$API_RETRY_DELAY"
    
    # Build base curl command
    local curl_base=()
    api_curl_cmd "$endpoint" curl_base
    
    # Add auth if provided
    if [[ -n "$api_key" ]]; then
        curl_base+=(-H "Authorization: Bearer ${api_key}")
    fi
    
    # Add payload
    curl_base+=(-d "@${payload_file}")
    
    # Add endpoint and timeout
    curl_base+=("$endpoint")
    curl_base+=(--max-time "$timeout")
    
    local attempt=1
    local response
    local http_code
    local exit_code
    
    while [[ $attempt -le $max_retries ]]; do
        # Capture both response and HTTP code
        response=$("${curl_base[@]}" -w "\n%{http_code}" 2>&1)
        exit_code=$?
        
        # Extract HTTP code (last line)
        http_code=$(echo "$response" | tail -n1)
        
        # Extract body (everything except last line)
        response=$(echo "$response" | head -n -1)
        
        # Check success conditions
        if [[ $exit_code -eq 0 && "$http_code" == "200" ]]; then
            echo "$response"
            return 0
        fi
        
        # Log failure if verbose
        if [[ "${_verbose_level:-0}" -ge 1 ]]; then
            log "API request failed:" -2
            log "  Endpoint: $endpoint" -2
            log "  HTTP Code: $http_code" -2
            log "  Exit Code: $exit_code" -2
            log "  Attempt: $attempt/$max_retries" -2
            if [[ ${#response} -lt 500 ]]; then
                log "  Response: ${response}" -2
            else
                log "  Response (first 500 chars): ${response:0:500}..." -2
            fi
        fi
        
        # Don't retry on certain errors
        if [[ "$http_code" =~ ^4[0-9]{2}$ && "$http_code" != "429" ]]; then
            # 4xx errors (except 429 rate limit) are client errors, don't retry
            break
        fi
        
        # Wait before retry with exponential backoff
        if [[ $attempt -lt $max_retries ]]; then
            sleep $retry_delay
            retry_delay=$((retry_delay * 2))
        fi
        
        ((attempt++))
    done
    
    # All retries exhausted or non-retryable error
    echo "$response"
    return 1
}

# Make streaming API request
# Usage: api_request_stream <endpoint> <payload_file> <api_key> <callback_function>
# Callback receives each line of SSE data
api_request_stream() {
    local endpoint="$1"
    local payload_file="$2"
    local api_key="$3"
    local callback="$4"
    
    # Build base curl command
    local curl_base=()
    api_curl_cmd "$endpoint" curl_base
    
    # Add auth if provided
    if [[ -n "$api_key" ]]; then
        curl_base+=(-H "Authorization: Bearer ${api_key}")
    fi
    
    # Add payload and streaming options
    curl_base+=(-d "@${payload_file}")
    curl_base+=(-N)  # No buffer for streaming
    curl_base+=("$endpoint")
    
    # Execute and process stream
    "${curl_base[@]}" 2>/dev/null | while IFS= read -r line || [[ -n "$line" ]]; do
        $callback "$line"
    done
    
    return "${PIPESTATUS[0]}"
}

# Cleanup connection pool
api_cleanup_pool() {
    if [[ -d "$_API_CONN_POOL_DIR" ]]; then
        rm -rf "$_API_CONN_POOL_DIR"
    fi
}

# Get connection pool stats for debugging
api_pool_stats() {
    local conn_count=0
    if [[ -d "$_API_CONN_POOL_DIR" ]]; then
        conn_count=$(find "$_API_CONN_POOL_DIR" -name "*.jar" 2>/dev/null | wc -l)
    fi
    echo "Active connections: $conn_count"
    echo "Pool directory: $_API_CONN_POOL_DIR"
}

# ============================================================================
# Streaming Optimizer - 16ms batching for 60fps target
# ============================================================================

# Use configured batch interval
_STREAM_BATCH_INTERVAL=$STREAM_BATCH_INTERVAL_NS

# Buffer for batched content
_stream_buffer=""
_last_flush_time=0

# Initialize streaming optimizer
stream_optimizer_init() {
    _stream_buffer=""
    _last_flush_time=$(date +%s%N)
}

# Add content to buffer
# Usage: stream_buffer_add <content>
stream_buffer_add() {
    local content="$1"
    _stream_buffer+="$content"
}

# Check if it's time to flush (60fps = ~16ms)
# Usage: stream_should_flush
stream_should_flush() {
    local current_time
    current_time=$(date +%s%N)
    local elapsed=$((current_time - _last_flush_time))
    
    [[ $elapsed -ge $_STREAM_BATCH_INTERVAL ]]
}

# Flush buffered content
# Usage: stream_buffer_flush <output_callback>
stream_buffer_flush() {
    local callback="$1"
    
    if [[ -n "$_stream_buffer" ]]; then
        $callback "$_stream_buffer"
        _stream_buffer=""
        _last_flush_time=$(date +%s%N)
    fi
}

# Optimized streaming request with 16ms batching
# Usage: api_request_stream_optimized <endpoint> <payload_file> <api_key> <content_callback>
# content_callback receives batched content every ~16ms
api_request_stream_optimized() {
    local endpoint="$1"
    local payload_file="$2"
    local api_key="$3"
    local content_callback="$4"
    local parse_fn="${5:-provider_parse_stream_chunk}"
    
    # Initialize optimizer
    stream_optimizer_init
    
    # Build base curl command
    local curl_base=()
    api_curl_cmd "$endpoint" curl_base
    
    # Add auth if provided
    if [[ -n "$api_key" ]]; then
        curl_base+=(-H "Authorization: Bearer ${api_key}")
    fi
    
    # Add payload and streaming options
    curl_base+=(-d "@${payload_file}")
    curl_base+=(-N)  # No buffer for streaming
    curl_base+=("$endpoint")
    
    # Execute and process stream with batching
    "${curl_base[@]}" 2>/dev/null | while IFS= read -r line || [[ -n "$line" ]]; do
        # Parse SSE data
        if [[ "${line:0:5}" == "data:" ]]; then
            local data="${line#data: }"
            
            if [[ "$data" != "" && "$data" != "[DONE]" ]]; then
                # Parse content from chunk
                local content
                content=$($parse_fn "$data" 2>/dev/null)
                
                if [[ -n "$content" ]]; then
                    stream_buffer_add "$content"
                    
                    # Flush if we've hit the 16ms target
                    if stream_should_flush; then
                        stream_buffer_flush "$content_callback"
                    fi
                fi
            fi
        fi
    done
    
    # Final flush of any remaining content
    stream_buffer_flush "$content_callback"
    
    return "${PIPESTATUS[0]}"
}
